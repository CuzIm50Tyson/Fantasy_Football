{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from datetime import datetime\n",
    "from espn_data.get_espn_data import weeks_since_start_season\n",
    "\n",
    "\n",
    "\n",
    "def split(df, col):\n",
    "    return [x for _, x in df.groupby(col)]\n",
    "\n",
    "ff2020 = pd.read_csv(\"fantasy/fantasy_data_2020.csv\")\n",
    "ff2021 = pd.read_csv(\"fantasy/fantasy_data_2021.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def create_top6_dict(ffdata):\n",
    "    top6_wins = ffdata[['team_name', 'week', 'top6_win']].drop_duplicates()\n",
    "    top6_wins.top6_win = top6_wins.top6_win.astype('int')\n",
    "    top6_split = split(top6_wins, 'team_name')\n",
    "\n",
    "    top6_dict = {}\n",
    "    for i in top6_split:\n",
    "        top6_dict.update({i.team_name.unique()[0]:list(i.top6_win.values)})\n",
    "    return top6_dict\n",
    "    \n",
    "def create_team_dict(raw_scores):\n",
    "    '''Create dictionary of team names and their scores'''\n",
    "    team_scores = split(raw_scores, 'team_name')\n",
    "    team_dict = {}\n",
    "    for i in team_scores:\n",
    "        team_name = i.team_name.drop_duplicates().values[0]\n",
    "        team_dict.update({team_name:i.points.values})\n",
    "    return team_dict\n",
    "\n",
    "def simulate_season(team_name, team_dict, top6_dict):\n",
    "    opponents = [i for i in list(team_dict.keys()) if i != team_name]\n",
    "    # Randomize order of opponents\n",
    "\n",
    "    # Get maximum number of games so far\n",
    "    games_count = len(team_dict[team_name])\n",
    "\n",
    "    all_wins = []\n",
    "    for i in range(10000):\n",
    "        new_order = random.sample(opponents, games_count) \n",
    "        wins = 0\n",
    "        for j, opp in enumerate(new_order):\n",
    "            my_points = team_dict[team_name][j]\n",
    "            opp_points = team_dict[opp][j]\n",
    "            if my_points > opp_points:\n",
    "                wins += 1\n",
    "            wins += top6_dict[team_name][j]\n",
    "        all_wins.append(wins)\n",
    "    return all_wins\n",
    "\n",
    "def build_probability_distribution(ffdata):\n",
    "    raw_scores = ffdata[['team_name', 'week', 'points']].drop_duplicates()\n",
    "\n",
    "    team_dict = create_team_dict(raw_scores)\n",
    "    top6_dict = create_top6_dict(ffdata)\n",
    "    all_team_names = sorted(list(set(raw_scores.team_name.values)))\n",
    "    simulated_wins = []\n",
    "    for name in all_team_names:\n",
    "        total_wins = pd.Series(simulate_season(name, team_dict, top6_dict))\n",
    "        total_wins = total_wins.rename(name)\n",
    "        simulated_wins.append(total_wins.value_counts() / total_wins.value_counts().sum())\n",
    "\n",
    "    return pd.concat(simulated_wins, axis=1).fillna(0)\n",
    "\n",
    "liklihood_table = build_probability_distribution(ff2021).sort_index(ascending=False).cumsum(axis=0).sort_index(ascending=True)\n",
    "format_dict = {col:'{:,.1%}'.format for col in liklihood_table.columns }\n",
    "\n",
    "alt.Chart(cusum_wins.reset_index()).mark_line().encode(x='index', y='probability', tooltip=[alt.Tooltip('index', title='Total H2H Wins'), 'probability']).interactive()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from espn_data.get_espn_data import our_league\n",
    "our_league"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "League(443750, 2021)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "def player_df_from_line(lineup, first_matchup, week, home_team):\n",
    "    '''Given a played lineup, return the player information'''\n",
    "    team_info_list = []\n",
    "    for player in lineup :\n",
    "        player_info_list = []\n",
    "\n",
    "        # Set team name\n",
    "        if home_team:\n",
    "            player_info_list.append(first_matchup.home_team.owner)\n",
    "        else:\n",
    "            player_info_list.append(first_matchup.away_team.owner)\n",
    "\n",
    "        # Set player info\n",
    "        player_info_list.append(week)\n",
    "        player_info_list.append(player.name)\n",
    "        player_info_list.append(player.position)\n",
    "        player_info_list.append(player.slot_position)\n",
    "        player_info_list.append(player.projected_points)\n",
    "        player_info_list.append(player.points)\n",
    "\n",
    "        if home_team:\n",
    "            player_info_list.append(first_matchup.away_team.owner)\n",
    "        else:\n",
    "            player_info_list.append(first_matchup.home_team.owner)\n",
    "\n",
    "        team_info_list.append(player_info_list)\n",
    "\n",
    "    player_df_single_week = pd.DataFrame(team_info_list)\n",
    "    player_df_cols = ['team_name', 'week', 'player_name', 'player_pos','player_slot', 'player_proj_points', 'player_points', 'opponent']\n",
    "    player_df_single_week.columns = player_df_cols\n",
    "    sorted_players = player_df_single_week.sort_values('player_points', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return sorted_players\n",
    "\n",
    "# Calcualte the maximum points given the following allowed positions\n",
    "\n",
    "def position_sorter(column):\n",
    "    \"\"\"Custom Sort function to put them in the way ESPN displays them\"\"\"\n",
    "    positions = ['QB', 'RB', 'WR', 'TE', 'FLEX', 'D/ST']\n",
    "    correspondence = {team: order for order, team in enumerate(positions)}\n",
    "    return column.map(correspondence)\n",
    "\n",
    "\n",
    "def add_ideal_to_player_df(player_df):\n",
    "    '''Add a tag to the player to determine if they were an ideal pick for that week'''\n",
    "    positions = ['QB', 'RB1', 'RB2', 'WR1', 'WR2', 'TE', 'FLEX', 'D/ST']\n",
    "    ordered_played = player_df.iterrows()\n",
    "\n",
    "    found_positions = []\n",
    "    for i, pos in ordered_played:\n",
    "        for set_pos in positions:\n",
    "            if pos.player_pos in set_pos:\n",
    "                found_positions.append(pos)\n",
    "                positions.remove(set_pos)\n",
    "                break\n",
    "            elif set_pos == 'FLEX' and pos.player_pos in ['WR', 'RB', 'TE']:\n",
    "                found_positions.append(pos)\n",
    "                positions.remove(set_pos)\n",
    "                break\n",
    "\n",
    "    ideal_lineup = pd.concat(found_positions, axis=1).transpose().sort_values(by='player_pos', key=position_sorter)\n",
    "    ideal_lineup['ideal_player'] = True\n",
    "    comb_player_ideal = player_df.merge( ideal_lineup[['team_name', 'player_name', 'ideal_player']], how='left').fillna(False)\n",
    "    comb_player_ideal['played'] = comb_player_ideal.player_slot != 'BE'\n",
    "    return comb_player_ideal\n",
    "\n",
    "def build_matchup_player_dfs(matchup, week):\n",
    "    '''Create a dataframe that contains all the players and their ideal status for each matchup'''\n",
    "    \n",
    "    home_lineup = matchup.home_lineup\n",
    "    away_lineup = matchup.away_lineup\n",
    "\n",
    "    home_player_df = player_df_from_line(home_lineup, matchup, week, True)\n",
    "    away_player_df = player_df_from_line(away_lineup, matchup, week, False)\n",
    "\n",
    "    home_player_added_ideals = add_ideal_to_player_df(home_player_df)\n",
    "    away_player_added_ideals = add_ideal_to_player_df(away_player_df)\n",
    "\n",
    "    combined_set = pd.concat([home_player_added_ideals, away_player_added_ideals])\n",
    "    return combined_set\n",
    "\n",
    "## Build the player DF\n",
    "@st.experimental_memo\n",
    "def build_full_player_df(our_league):\n",
    "    '''Build a dataframe of teams and player information on each team'''\n",
    "    full_player_df = []\n",
    "    for week in range(1,weeks_since_start_season()+1):\n",
    "        matchups = our_league.box_scores(week)\n",
    "        for match in matchups:\n",
    "            full_player_df.append(build_matchup_player_dfs(match, week))\n",
    "\n",
    "    return pd.concat(full_player_df)\n",
    "\n",
    "@st.experimental_memo\n",
    "def waiver_table(our_league):\n",
    "    '''Create a table of teams, transaction, and player_names'''\n",
    "    activities = our_league.recent_activity(1000)\n",
    "    fa_adds = []\n",
    "    for activity in activities:\n",
    "        row = []\n",
    "        transaction_date = datetime.fromtimestamp(activity.date/1000).strftime('%Y-%m-%d %H:%M:%S.%f') \n",
    "        for step in activity.actions:\n",
    "            if 'FA ADDED' in step or 'WAIVER ADDED' in step:\n",
    "                row.append(transaction_date)\n",
    "                row.append(step[0].owner)\n",
    "                row.append(step[1])\n",
    "                row.append (step[2].name)\n",
    "                fa_adds.append(row)\n",
    "\n",
    "    return pd.DataFrame(fa_adds, columns=['date','team_name', 'action', 'player_name'])\n",
    "\n",
    "# Average waiver points by team\n",
    "def calc_avg_waiver_points_by_team(our_league):\n",
    "    '''Create a dataframe of team_name and average points for waiver addition'''\n",
    "    full_player_table = build_full_player_df(our_league)\n",
    "    transaction_table = waiver_table(our_league)\n",
    "\n",
    "    full_player_df_waivers = full_player_table.merge(transaction_table[['team_name', 'player_name', 'action']], how='left').fillna('DRAFTED')\n",
    "\n",
    "    avg_waiver_points = full_player_df_waivers.query('(action == \"WAIVER ADDED\" | action == \"FA ADDED\") & played == True').groupby(['team_name'], as_index=False).mean()[['team_name', 'player_points']]\n",
    "    avg_waiver_points = avg_waiver_points.rename(columns={'player_points':'average_waiver_points'})\n",
    "    return avg_waiver_points\n",
    "\n",
    "# Calculate win vs loss point differential\n",
    "def win_loss_marings(ffdata):\n",
    "    '''Create a dataframe of team, avg win margin, and avg loss margin'''\n",
    "    ffdata['point_diff'] = ffdata.points - ffdata.points_against\n",
    "    win_loss_diff_table = ffdata.groupby(['team_name', 'h2h_win'], as_index=False)['point_diff'].mean()\n",
    "    win_loss_pivot = win_loss_diff_table.pivot('team_name', 'h2h_win', 'point_diff').fillna('-')\n",
    "    win_loss_pivot = win_loss_pivot.reset_index().rename_axis(None, axis = 1)\n",
    "    win_loss_pivot.columns = ['team_name', 'avg_margin_of_loss', 'avg_margin_of_victor']\n",
    "    return win_loss_pivot\n",
    "\n",
    "# Joined teams, margins, and waiver points\n",
    "def calc_margins_waivers(fantasy_data, our_league):\n",
    "    '''Create a dataframe of teams, avg_loss_margin, avg_waiver_points'''\n",
    "    win_loss_pivot = win_loss_marings(fantasy_data)\n",
    "    avg_waiver_points = calc_avg_waiver_points_by_team(our_league)\n",
    "    return win_loss_pivot.merge(avg_waiver_points, how = 'outer').fillna(0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from espn_api.football import League\n",
    "import pickle\n",
    "import streamlit as st\n",
    "from espn_data.get_espn_data import get_2021_season_data\n",
    "\n",
    "for year in [2021]:\n",
    "    filename = f\"./fantasy/waiver_data/wd_{year}.pickle\"\n",
    "    our_league = League(\n",
    "            league_id=st.secrets[\"league_id\"], year=year, espn_s2=st.secrets[\"espn_s2\"], swid=st.secrets[\"swid\"]\n",
    "        )\n",
    "    get_2021_season_data(year).to_csv(f\"./fantasy/fantasy_data_{year}.csv\", index=False)\n",
    "    ra = our_league.recent_activity(2000)\n",
    "\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(ra, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from espn_data.get_espn_data import get_2021_season_data\n",
    "from espn_api.football import League\n",
    "import streamlit as st\n",
    "\n",
    "year = 2021\n",
    "our_league = League(\n",
    "            league_id=st.secrets[\"league_id\"], year=year, espn_s2=st.secrets[\"espn_s2\"], swid=st.secrets[\"swid\"]\n",
    "        )\n",
    "get_2021_season_data(year).to_csv(f\"./fantasy/fantasy_data_{year}.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from espn_api.football import League\n",
    "import pickle\n",
    "import streamlit as st\n",
    "from espn_data.get_espn_data import get_2021_season_data\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Read YAML file\n",
    "with open(\".env_vars.yaml\", 'r') as stream:\n",
    "    data_loaded = yaml.safe_load(stream)\n",
    "for key, value in data_loaded.items():\n",
    "    os.environ[key] = value\n",
    "\n",
    "for year in [2021]:\n",
    "\n",
    "    BUCKET_NAME = \"fantasy-football-palo-alto-data\"\n",
    "    PROJECT_NAME = \"fantasy-football-palo-alto\"\n",
    "\n",
    "    # Initalize league\n",
    "    our_league = League(\n",
    "        league_id=os.environ.get(\"league_id\"),\n",
    "        year=year,\n",
    "        espn_s2=os.environ.get(\"espn_s2\"),\n",
    "        swid=os.environ.get(\"swid\"),\n",
    "    )\n",
    "    logging.info(\"League connected\")\n",
    "\n",
    "    # Download data\n",
    "    season_data_2021 = get_2021_season_data(year, our_league)\n",
    "    logging.info(\"Data downloaded\")\n",
    "\n",
    "    # Connect to the client\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "    # Save the data\n",
    "    bucket.blob(f\"fantasy_data_{year}.csv\").upload_from_string(season_data_2021.to_csv(index=False), \"text/csv\")\n",
    "    logging.info(\"Data saved to GCS\")\n",
    "\n",
    "    # Download the waiver data\n",
    "    ra = our_league.recent_activity(2000)\n",
    "    fs = gcsfs.GCSFileSystem(project=)\n",
    "\n",
    "    logging.info(\"Waiver data downloaded\")\n",
    "\n",
    "    filename = f\"{BUCKET_NAME}/wd_{year}.pickle\"\n",
    "    with fs.open(filename, \"wb\") as handle:\n",
    "        pickle.dump(ra, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    logging.info(\"Waiver data saved\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-04 22:41:31.585 INFO    root: League connected\n",
      "2021-10-04 22:42:23.706 INFO    root: Data downloaded\n",
      "/Users/tjsimons/dev/fantasy/fantasy_streamlit/lib/python3.8/site-packages/google/auth/_default.py:68: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/tjsimons/dev/fantasy/fantasy_streamlit/lib/python3.8/site-packages/google/auth/_default.py:68: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "2021-10-04 22:42:26.809 INFO    root: Data saved to GCS\n",
      "/Users/tjsimons/dev/fantasy/fantasy_streamlit/lib/python3.8/site-packages/google/auth/_default.py:68: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "2021-10-04 22:42:39.902 INFO    root: Waiver data downloaded\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "Attempt to open a bucket",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6a78e5fe529c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"wd_{year}.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Waiver data saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fantasy/fantasy_streamlit/lib/python3.8/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fantasy/fantasy_streamlit/lib/python3.8/site-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, cache_options, acl, consistency, metadata, autocommit, **kwargs)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_block_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsistency\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return GCSFile(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fantasy/fantasy_streamlit/lib/python3.8/site-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gcsfs, path, mode, block_size, autocommit, cache_type, cache_options, acl, consistency, metadata, content_type, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to open a bucket\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcsfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcsfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Attempt to open a bucket"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('fantasy_streamlit': venv)"
  },
  "interpreter": {
   "hash": "73469b6626f9eb14479f062158c188179454edfad16f9bc66fd576ec36ae942c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}